## Festlegen der LLM-Strategie

Die rasche Verbreitung von Anwendungen auf Basis großer Sprachmodelle (LLM) hat die Aufmerksamkeit an und Untersuchung von allen KI/ML-Systemen, die in Geschäftsprozessen verwendet werden, verstärkt. Dies umfasst sowohl Generative AI als auch lang etablierte prädikative-KI/ML-Systeme. Diese erhöhte Fokussierung legt potenzielle Risiken offen, wie etwa Angreifer, die Systeme ins Visier nehmen und zuvor übersehen wurden, und Governance- oder rechtliche Herausforderungen, die möglicherweise in Bezug auf rechtliche, Datenschutz-, Haftungs- oder Garantiefragen bislang ignoriert wurden. Für jede Organisation, die KI/ML-Systeme in ihren Abläufen einsetzt, ist es entscheidend, umfassende Richtlinien, Governance, Sicherheitsvorgehen, Datenschutzmaßnahmen und Verantwortungsstandards zu bewerten und zu etablieren, um sicherzustellen, dass diese Technologien bezüglich Sicherheit und Ethik zu den Geschäftsprozessen passen.

Angreifer oder Gegner stellen die unmittelbarste und schädlichste Bedrohung für Unternehmen, Personen und Regierungsbehörden dar. Ihre Ziele reichen von finanziellem Gewinn bis hin zu Spionage und führen dazu, dass sie kritische Informationen stehlen, den Betrieb stören und das Vertrauen schädigen. Dazu erwerben sie durch KI und maschinelles Lernen die Möglichkeit, mit Geschwindigkeit und Raffinesse den Verteidigern voraus zu sein.

Die dringendste LLM-Bedrohung jenseits von Angreifern ist "Shadow AI": Mitarbeiter, die nicht genehmigte Online-KI-Tools, unsichere Browser-Plug-ins und Drittanbieteranwendungen verwenden, die LLM-Funktionen durch Updates oder Upgrades einführen und damit standardisierte Softwaregenehmigungsprozesse umgehen.

>||center|16|16 Bereitstellung

>cornflower|white|left|14|18 Schritt 1: Resilienz-Strategie zuerst

    >fidlightblue|black|left|12|16 ▶ Identifikation unmittelbarer Bedrohungen durch Bedrohungsmodellierung von Missbrauchsfällen
    >fidlightblue|black|left|12|16 ▶ Überprüfen Sie interne oder externe Ausnutzungsmöglichkeiten in den Bedrohungsmodellszenarien und verifizieren Sie Sicherheitsmaßnahmen
    >fidlightblue|black|left|12|16 ▶ Scannen und überwachen Sie Ihr Environment auf Vorkommen von Schadsoftware

>dodgerblue|white|left|14|18 Schritt 2: Bestehende Richtlinien aktualisieren

    >fidlightblue|black|left|12|16 ▶ Verträge, Geheimhaltungsvereinbarungen, Governance und Sicherheit prüfen, um bei Bedarf Nutzung und Bedrohung durch LLMs oder GenAI zu integrieren

>fidblue|white|left|14|18 Schritt 3: Schulung / Bildung

    >fidlightblue|black|left|12|16 ▶ Aktualisieren Sie das Security-Awareness-Training, Ihre Entwickler-, rechtliche oder andere Schulungen, um die Nutzung oder Bedrohung durch LLMs oder GenAI einzubeziehen

>darkblue|white|left|14|18 Schritt 4: Führungskräfte einbeziehen

    >fidlightblue|black|left|12|16 ▶ Arbeiten Sie mit Führungskräften und anderen Stakeholdern zusammen, um LLM- oder GenAI-Lösungsstrategien zu identifizieren
    >fidlightblue|black|left|12|16 ▶ Implementieren Sie eine Risikomanagementstrategie

>cornflower|white|left|14|18 Schritt 5: Aktualisieren Sie Ihr Programm zur Risikomanagement bei Drittanbietern

    >fidlightblue|black|left|12|16 ▶ Lösungen von Drittanbietern und Anbietern für KI benötigen erweiterte Fragebögen und Überprüfungen

>dodgerblue|white|left|14|18 Schritt 6: Eine Bereitstellungsstrategie wählen

##### Abbildung 2.1 Optionen für Bereitstellungsstrategie
>white|black|center Quelle: sdunn


### Bereitstellungsstrategie

Das Spektrum reicht hier von der Nutzung öffentlicher Endverbraucheranwendungen bis zum Training proprietärer Modelle auf eigenen Daten. Faktoren wie Sensibilität der Anwendungsfälle, die benötigten Fähigkeiten und verfügbare Ressourcen helfen dabei, das passende Gleichgewicht zwischen Komfort und Kontrolle zu bestimmen. Das Verständnis der fünf Modelltypen bietet dabei einen Rahmen für die Bewertung der Optionen.

>||center|16|16 Bereitstellungstypen

>cornflower|white|left|14|18 Typ 1: Direkte Nutzung

    >fidlightblue|black|left|12|16 ▶ Große Sprachmodelle über die Nutzerschnittstelle nutzen
    >fidlightblue|black|left|12|16 ▶ Risikoreduktion durch Unternehmensrichtlinien und Mitarbeitertraining
    >fidlightblue|black|left|12|16 ▶ Vorteile: Flexibles und schnelles Experimentieren
    >fidlightblue|black|left|12|16 ▶ Beispiele: Perplexity, ChatGPT, big-AGI

>dodgerblue|white|left|14|18 Typ 2: Zugriff über Model API

    >fidlightblue|black|left|12|16 ▶ Große Sprachmodelle direkt bei den Anbietern über deren API nutzen
    >fidlightblue|black|left|12|16 ▶ Risikoreduktion durch Unternehmensrichtlinien und Mitarbeitertraining
    >fidlightblue|black|left|12|16 ▶ Vorteile: Schnelles Experimentieren mit etwas zentraler Kontrolle über die API
    >fidlightblue|black|left|12|16 ▶ Beispiele: Claude, ChatGPT, Gemini

>fidblue|white|left|14|18 Typ 3: Lizenzmodell

    >fidlightblue|black|left|12|16 ▶ Ein lizenziertes großes Sprachmodell auf den eigenen Systemen betrieben
    >fidlightblue|black|left|12|16 ▶ Risikoreduktion durch Eigenbetrieb, durch Unternehmensrichtlinien und Mitarbeitertraining 
    >fidlightblue|black|left|12|16 ▶ Vorteile: mehr Kontrolle und bessere Integration durch internen Tools und Workflows
    >fidlightblue|black|left|12|16 ▶ Beispiele: Microsoft Enterprise CoPilot, Amazon Codewhisperer, SalesForce Einstein GPT

>darkblue|white|left|14|18 Typ 4: Vortrainiertes Modell nutzen

    >fidlightblue|black|left|12|16 ▶ Ein allgemeines Basismodell nutzen, das durch Finetuning mit Unternehmens- oder benutzerdefinierten Daten anpasst wurde
    >fidlightblue|black|left|12|16 ▶ Risikoreduktion durch erhöhte Transparenz, Unternehmensrichtlinien und Mitarbeitertraining
    >fidlightblue|black|left|12|16 ▶ Vorteile: Verbesserte Leistung und reduzierte Halluzinationen
    >fidlightblue|black|left|12|16 ▶ Beispiele: QwenLM/Qwen 1.5, DBRX, Starling 7B

>cornflower|white|left|14|18 Typ 5: Bewährtes Modell finetunen

    >fidlightblue|black|left|12|16 ▶ Bewährte (spezialisierte) Modelle nutzen und weiter mit eigenen Daten finetunen, um sie an Ihr Unternehmen anzupassen
    >fidlightblue|black|left|12|16 ▶ Risikoreduktion durch erhöhter Transparenz, verbesserte Leistung, reduzierte Halluzinationen,Unternehmensrichtlinien und Mitarbeitertraining 
    >fidlightblue|black|left|12|16 ▶ Vorteile: Anpassung über die vortrainierten Modelle hinaus
    >fidlightblue|black|left|12|16 ▶ Beispiele: Google MedPalm, Amazon Bedrock, Llama2, LegalAI

>dodgerblue|white|left|14|18 Typ 6: Individuelle Modelle

    >fidlightblue|black|left|12|16 ▶ Eine maßgeschneiderte KI/ML-Modellarchitektur für Ihren spezifischen Unternehmensanwendungsfall bauen
    >fidlightblue|black|left|12|16 ▶ Risikoreduktion durch vollständige Sichtbarkeit und Kontrolle, durch Unternehmensrichtlinien, Entwickler- und Mitarbeitertraining 
    >fidlightblue|black|left|12|16 ▶ Vorteile: Erfordert große Investitionen, maximiert aber die Anpassbarkeit

##### Abbildung 2.2 Optionen für Bereitstellungstypen
>white|black|center Quelle: sdunn