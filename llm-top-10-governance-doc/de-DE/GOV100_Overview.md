## Übersicht

Jeder Internetnutzer und jedes Unternehmen muss sich auf die kommende Welle leistungsfähiger generativer künstlicher Intelligenz (GenAI) Anwendungen vorbereiten. GenAI verspricht enorme Innovationen, Effizienz und kommerzielle Vorteile in einer Vielzahl von Branchen. Doch wie jede leistungsstarke Technologie in einem frühen Entwicklungsstadium birgt sie ihre eigenen offensichtlichen und unerwarteten Herausforderungen.

Künstliche Intelligenz hat in den letzten 50 Jahren große Fortschritte gemacht und eine Vielzahl von Unternehmensprozessen unauffällig unterstützt, bis das öffentliche Erscheinen von ChatGPT die Entwicklung und Nutzung von Large Language Models (LLMs) sowohl bei Einzelpersonen als auch bei Unternehmen vorangetrieben hat. Anfangs waren diese Technologien auf akademische Studien oder die Ausführung bestimmter, aber wichtiger Aktivitäten innerhalb von Unternehmen beschränkt und nur für wenige Auserwählte sichtbar. Die jüngsten Fortschritte in der Verfügbarkeit von Daten, Computerleistung, GenAI-Fähigkeiten und die Veröffentlichung von Tools wie Llama 2, ElevenLabs und Midjourney haben AI jedoch von einer Nische zu einer allgemein weitverbreiteten Akzeptanz gebracht. Diese Verbesserungen haben nicht nur die GenAI-Technologien zugänglicher gemacht, sondern auch den kritischen Bedarf für Unternehmen hervorgehoben, solide Strategien für die Integration und Nutzung von AI in ihren Geschäftstätigkeiten zu entwickeln, was einen großen Schritt nach vorn in der Art und Weise darstellt, wie wir Technologie nutzen.

  - Künstliche Intelligenz ist ein breiter Begriff, der alle Bereiche der Informatik umfasst, die es Maschinen ermöglichen, Aufgaben auszuführen, die normalerweise menschliche Intelligenz erfordern würden. Maschinelles Lernen und generative AI sind zwei Unterkategorien von KI.
  - Maschinelles Lernen ist eine Unterkategorie von KI, die sich auf die Erstellung von Algorithmen konzentriert, die aus Daten lernen können. Maschinelle Lernalgorithmen werden mit einem Datensatz trainiert und können dann diese Daten verwenden, um Vorhersagen oder Entscheidungen über neue Daten zu treffen.
  - Generative AI ist eine Art von maschinellem Lernen, die sich auf die Erstellung neuer Daten konzentriert. Oftmals stützt sich GenAI auf die Verwendung von Large Language Models, um die erforderlichen Aufgaben zur Erstellung der neuen Daten durchzuführen.
  - Ein Large Language Model (LLM) ist eine Art von KI-Modell, das menschenähnlichen Text verarbeitet und generiert. Im Kontext der künstlichen Intelligenz bezieht sich ein "Modell" auf ein System, das trainiert wurde, um Vorhersagen basierend auf Eingabedaten zu treffen. LLMs werden speziell auf großen Datensätzen natürlicher Sprache trainiert, daher der Name Large Language Models.

Organisationen betreten Neuland bei der Sicherung und Überwachung von GenAI-Lösungen. Der schnelle Fortschritt von GenAI eröffnet auch Türen für Gegner, ihre Angriffsstrategien zu verbessern und stellt eine doppelte Herausforderung aus Verteidigung und Bedrohungseskalation dar.

Unternehmen verwenden künstliche Intelligenz in vielen Bereichen, einschließlich HR, für  Recruiting, E-Mail-Spam-Filterung, SIEM für Verhaltensanalysen und Managed Detection and Response-Anwendungen. Der Schwerpunkt dieses Dokuments liegt jedoch auf Large Language Model-Anwendungen und ihrer Funktion bei der Erstellung generierter Inhalte.

### Verantwortungsvolle und vertrauenswürdige künstliche Intelligenz

Da Herausforderungen und Vorteile der künstlichen Intelligenz entstehen – und Vorschriften und Gesetze verabschiedet werden – entwickeln sich die Prinzipien und Säulen der verantwortungsvollen und vertrauenswürdigen KI-Nutzung von idealistischen Objekten und Bedenken kommend zu etablierten Standards.

[Die OWASP AI Exchange Working Group](https://owasp-ai-exchange.web.app/) verfolgt diese Veränderungen und geht die breiteren und herausfordernderen Überlegungen für alle Aspekte der künstlichen Intelligenz an.

>||center|16|16 Vertrauenswürdige künstliche Intelligenz

>indianred|white|left|14|18 ZUVERLÄSSIG
    >indianred|white|left|12|16 Robust
    >indianred|white|left|12|16 Verantwortlich
    >indianred|white|left|12|16 Überwacht
    >indianred|white|left|12|16 Transparent
    >indianred|white|left|12|16 Erklärbar

>forestgreen|white|left|14|18 RESILIENT
    >forestgreen|white|left|12|16 Sicher
    >forestgreen|white|left|12|16 Geschützt
    >forestgreen|white|left|12|16 Privat
    >forestgreen|white|left|12|16 Effektiv

>dodgerblue|white|left|14|18 VERANTWORTUNGSVOLL
    >dodgerblue|white|left|12|16 Fair
    >dodgerblue|white|left|12|16 Ethisch
    >dodgerblue|white|left|12|16 Inklusiv
    >dodgerblue|white|left|12|16 Nachhaltig
    >dodgerblue|white|left|12|16 Zweckmäßig

##### Abbildung 1.1 Säulen der vertrauenswürdigen künstlichen Intelligenz
##### erstellt vom Montreal Ethics Institute Beispiel

### Für wen ist dies?

Die OWASP Top 10 für LLM-Anwendungen-Cybersecurity-und-Governance-Checkliste ist für Führungskräfte aus der Exekutive, Technik, Cybersicherheit, Datenschutz, Compliance und Recht, DevSecOps, MLSecOps und Cybersicherheitsteams und Verteidiger gedacht. Sie richtet sich an Personen, die bestrebt sind, in der schnelllebigen KI-Welt einen Schritt voraus zu sein, mit dem Ziel, KI nicht nur für den Unternehmenserfolg zu nutzen, sondern auch gegen die Risiken überstürzter oder unsicherer KI-Implementierungen zu schützen. Diese Führungskräfte und Teams müssen Taktiken entwickeln, um Chancen zu ergreifen, Herausforderungen zu bekämpfen und Risiken zu mindern.

Diese Checkliste soll diesen Technologie- und Geschäftsführern dabei helfen, schnell die Risiken und Vorteile der Nutzung von LLM zu verstehen und sich auf die Entwicklung einer umfassenden Liste kritischer Bereiche und Aufgaben zu konzentrieren, die benötigt werden, um die Organisation zu verteidigen und zu schützen, während sie eine Large Language Model-Strategie entwickeln.

Es ist die Hoffnung des OWASP Top 10 für das LLM-Anwendungen-Teams, dass diese Liste Organisationen helfen wird, ihre bestehenden Verteidigungstechniken zu verbessern und Techniken zu entwickeln, um die neuen Bedrohungen anzugehen, die aus der Nutzung dieser aufregenden Technologie resultieren.

### Warum eine Checkliste?

Checklisten, die zur Formulierung von Strategien verwendet werden, verbessern die Genauigkeit, definieren Ziele, bewahren die Einheitlichkeit und fördern zielgerichtetes, bedachtes Arbeiten, wodurch das Übersehen und Verpassen von Details reduziert werden. Die Nutzung einer Checkliste erhöht nicht nur das Vertrauen in eine sichere Einführung, sondern fördert auch zukünftige Organisationsinnovationen, indem sie eine einfache und effektive Möglichkeit für kontinuierliche Verbesserungen bietet.

### Nicht umfassend

Obwohl dieses Dokument Organisationen dabei unterstützen soll, eine anfängliche LLM-Strategie in einer sich schnell ändernden technischen, rechtlichen und regulatorischen Umgebung zu entwickeln, ist es nicht erschöpfend und deckt nicht jeden Anwendungsfall oder jede Verpflichtung ab. Die Nutzung dieses Dokuments soll Organisationen dazu anregen, Bewertungen und Praktiken über den Umfang der bereitgestellten Checkliste hinaus zu erweitern, wie es für ihren Anwendungsfall oder ihre Zuständigkeit erforderlich ist.

### Die Herausforderungen von Large Language Models

Large Language Models stehen vor mehreren ernsthaften und einzigartigen Problemen. Eines der wichtigsten ist, dass beim Arbeiten mit LLMs die Steuerungs- und Datenebenen nicht strikt isoliert oder trennbar sind. Eine weitere wichtige Herausforderung ist, dass LLMs von Natur aus nichtdeterministisch sind und bei Aufforderung oder Anfrage ein anderes Ergebnis liefern. LLMs verwenden semantische Suche anstelle von Schlüsselwortsuche. Der Kernunterschied zwischen den beiden ist, dass der Algorithmus des Modells die Begriffe in seiner Antwort priorisiert. Dies ist eine bedeutende Abweichung von der Art und Weise, wie Verbraucher bisher Technologie genutzt haben, und es hat Auswirkungen auf die Konsistenz und Zuverlässigkeit der Ergebnisse. Halluzinationen, die aus den Lücken und Trainingsfehlern in den Daten resultieren, auf denen das Modell trainiert wurde, sind das Ergebnis dieser Methode.

Es gibt Methoden, um die Zuverlässigkeit zu verbessern und die Angriffsfläche für Jailbreaking, Modelltäuschungen und Halluzinationen zu reduzieren, aber es gibt einen Kompromiss zwischen Einschränkungen und Nutzen sowohl in Kosten als auch in Funktionalität.

Die Nutzung und Anwendungen von LLM erhöhen die Angriffsfläche einer Organisation. Einige Risiken der Risiken von LLM-Applikationen sind einzigartig, aber viele sind bekannte Probleme, wie die bekannte Software Bill of Materials (SBoM), die Lieferkette, Data Loss Protection (DLP) und unautorisierter Zugriff. Es gibt auch erhöhte Risiken, die nicht direkt mit GenAI zusammenhängen, bei denen GenAI die Effizienz, Fähigkeit und Wirksamkeit von Angreifern, die Organisationen, Einzelpersonen und Regierungssysteme angreifen und bedrohen erhöht.

Gegner nutzen zunehmend LLM- und Generative AI-Tools, um traditionelle Methoden des Angriffs auf Organisationen, Einzelpersonen und Regierungssysteme zu verbessern und zu beschleunigen. LLM verbessert ihre Fähigkeit, Techniken zu verbessern, um mühelos neue Malware zu erstellen, die möglicherweise mit neuartigen Zero-Day-Schwachstellen eingebettet ist oder darauf ausgelegt ist, die Erkennung zu umgehen. Sie können auch ausgeklügelte, einzigartige oder maßgeschneiderte Phishing-Schemata generieren. Die Erstellung überzeugender Deepfakes, sei es Video oder Audio, vereinfacht Social-Engineering. Zusätzlich vereinfachen diese Tools Einbrüche und innovative Hacking-Fähigkeiten zu entwickeln. In Zukunft wird der „maßgeschneiderte“ und kombinierte Einsatz von KI-Technologie durch kriminelle Akteure spezifische Antworten und dedizierte Lösungen für die angemessene Verteidigung und Widerstandsfähigkeit der Organisationen erfordern.

Organisationen gehen auch ein Risiko ein,wenn sie die Fähigkeiten von LLMs NICHT zu nutzen, haben einen Wettbewerbsnachteil. Sie werden auf dem Markt von Kunden und Partnern als veraltet, unfähig zum Skalieren personalisierter Kommunikation wahrgenommen. Ihnen wird Innovationsstagnation, betriebliche Ineffizienz, ein höheres Risiko menschlicher Fehler in Prozessen und ineffiziente Zuweisung von Humanressourcen unterstellt.

Das Verständnis der verschiedenen Arten von Bedrohungen und deren Integration in die Geschäftsstrategie hilft dabei, sowohl die Vor- als auch die Nachteile der Nutzung von Large Language Models (LLMs) gegenüber deren Nichtnutzung abzuwägen und sicherzustellen, dass sie die Geschäftsziele beschleunigen und nicht behindern.

### LLM-Bedrohungskategorien

>||center|16|16 KI-Bedrohungskarte

    >cornflower|white|left|14|18 Bedrohungen durch NICHT-Nutzung von KI-Modellen
    >cornflower|white|left|14|18 Bedrohungen durch Nutzung von KI-Modellen
    >dodgerblue|white|left|14|18 Bedrohungen für KI-Modelle
    >fidblue|white|left|14|18 Bedrohungen durch KI-Modelle
    >darkblue|white|left|14|18 KI-rechtliche & regulatorische Bedrohungen

##### Abbildung 1.2 Arten von KI-Bedrohungen
##### credit sdunn



### Schulung zu Sicherheit und Datenschutz in der künstlichen Intelligenz

Mitarbeiter in Organisationen profitieren von Schulungen, um künstliche Intelligenz, generative künstliche Intelligenz und die zukünftigen potenziellen Konsequenzen des Aufbaus, Kaufs oder der Nutzung von LLMs zu verstehen. Schulungen für zulässige Nutzung und Sicherheitsbewusstsein sollten sich sowohl an alle Mitarbeiter richten als auch spezialisierter für bestimmte Positionen wie Personalwesen, Rechtsabteilung, Entwickler, Datenteams und Sicherheitsteams sein.

Richtlinien für faire Nutzung und gesunde Interaktion sind Schlüsselaspekte, die, wenn sie von Anfang an integriert werden, ein Eckpfeiler für den Erfolg zukünftiger KI-Cybersicherheitsbewusstseinskampagnen sein werden. Dies wird den Benutzern das Wissen über die grundlegenden Regeln für die Interaktion sowie die Fähigkeit vermitteln, gutes von schlechtem oder unethischem Verhalten zu unterscheiden.

### Integration von LLM-Sicherheit und -Governance mit bestehenden, etablierten
### Praktiken und Kontrollen

Obwohl KI und generative KI eine neue Dimension zu Cybersicherheit, Resilienz, Datenschutz und der Erfüllung rechtlicher und regulatorischer Anforderungen hinzufügen, sind die lang existierenden bewährten Praktiken immer noch der beste Weg, um Probleme zu identifizieren, Schwachstellen zu finden, diese zu beheben und potenzielle Sicherheitsprobleme zu reduzieren.

  ● Stellen Sie sicher, dass das Management von KI-Systemen mit bestehenden organisatorischen Praktiken integriert ist.
  ● Stellen Sie sicher, dass KI/ML-Systeme bestehenden Datenschutz-, Governance- und Sicherheitspraktiken folgen und bei Bedarf spezifische KI-Datenschutz-, Governance- und Sicherheitspraktiken implementiert werden.

### Grundlegende Sicherheitsprinzipien

LLM-Fähigkeiten führen einen anderen Typ von Angriff und Angriffsfläche ein. LLMs sind anfällig für komplexe Geschäftslogikfehler, wie z. B. Prompt-Injektion, unsicheres Plugin-Design und Remote-Code-Ausführung. Existierende Best Practices sind der beste Weg, um diese Probleme zu lösen. Ein internes Produktsicherheitsteam, das sichere Softwareüberprüfung, Architektur, Data-Governance und Drittanbieterbewertungen versteht, muss überprüfen, wie stark die aktuellen Mechanismen sind, um Probleme zu finden, die durch LLM verschlimmert werden könnten, wie z. B. Stimmklonung, Personifizierung oder das Umgehen von Captchas.

Angesichts der jüngsten Fortschritte im maschinellen Lernen, NLP (Natural Language Processing), NLU (Natural Language Understanding), Deep Learning und in jüngerer Zeit LLMs (Large Language Models) und Generative AI wird empfohlen, Fachleute, die in diesen Bereichen versiert sind, bei Cybersicherheits- und Devops-Teams einzubeziehen. Ihre Expertise wird nicht nur bei der Einführung dieser Technologien, sondern auch bei der Entwicklung innovativer Analysen und Antworten auf neu entstehende Herausforderungen helfen.

### Risiko

Der Verweis auf Risiko verwendet die Definition as ISO 31000: Risiko = "Effekt der Unsicherheit auf Ziele." Zu den in der Checkliste enthaltenen LLM-Risiken gehört eine gezielte Liste von LLM-Risiken, die gegnerische, sicherheitstechnische, rechtliche, regulatorische, rufschädigende, finanzielle und wettbewerbliche Risiken ansprechen.

### Schwachstellen- und Minderungstaxonomie

Aktuelle Systeme zur Klassifizierung von Schwachstellen und zum Austausch von Bedrohungsinformationen, wie etwa OVAL, STIX, CVE und CWE, arbeiten noch an der  Fähigkeit, spezifische Schwachstellen und Bedrohungen in Bezug auf große Sprachmodelle (LLMs) und Prediction-Modelle zu überwachen und die Verteidiger zu alarmieren. Es wird erwartet, dass Organisationen sich auf diese etablierten und anerkannten Standards wie CVE für die Klassifizierung von Schwachstellen und STIX für den Austausch von Informationen über Cyber-Bedrohungen (CTI) stützen werden, wenn Schwachstellen oder Bedrohungen für KI/ML-Systeme und deren Lieferketten identifiziert werden.